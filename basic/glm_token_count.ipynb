{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "015d8a47f06f4f77a292f3743e8beff9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/1.41k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A new version of the following files was downloaded from https://huggingface.co/THUDM/chatglm3-6b:\n",
      "- tokenization_chatglm.py\n",
      ". Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f8d9fd0967354ea78227f0db7af8f2c0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/3.00 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting eos_token is not supported, use the default one.\n",
      "Setting pad_token is not supported, use the default one.\n",
      "Setting unk_token is not supported, use the default one.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"THUDM/chatglm3-6b\", trust_remote_code=True, encode_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count Token:7\n",
      "Token IDs:[64790, 64792, 985, 323, 260, 2254, 16948]\n",
      "Decode sentence:['[gMASK]', 'sop', '▁here', '▁is', '▁a', '▁text', '▁demo']\t"
     ]
    }
   ],
   "source": [
    "def count_encode(inputs:str=\"\"):\n",
    "    encoded_input=tokenizer.encode(inputs)\n",
    "    num_tokens=len(encoded_input)\n",
    "    return encoded_input,num_tokens\n",
    "\n",
    "def decode(inputs:list=[]):\n",
    "    decode_sentence=tokenizer.convert_ids_to_tokens(inputs)\n",
    "    return decode_sentence\n",
    "\n",
    "text=\"here is a text demo\"\n",
    "encoded_input,num_tokens=count_encode(text)\n",
    "print(f'Count Token:{num_tokens}')\n",
    "print(f'Token IDs:{encoded_input}')\n",
    "decode_sentences=decode(encoded_input)\n",
    "print(f\"Decode sentence:{decode_sentences}\",end=\"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count Token:11\n",
      "Token IDs:[64790, 64792, 64795, 30910, 13, 985, 323, 260, 2254, 16948, 64796]\n",
      "['[gMASK]', 'sop', '<|user|>', '▁', '<0x0A>', '▁here', '▁is', '▁a', '▁text', '▁demo', '<|assistant|>']\t"
     ]
    }
   ],
   "source": [
    "messages=[]\n",
    "messages.append({\n",
    "    \"role\":\"user\",\n",
    "    \"content\":\"here is a text demo\"\n",
    "})\n",
    "\n",
    "chat_inputs=tokenizer.apply_chat_template(messages,add_generation_prompt=True,tokenize=True)\n",
    "num_tokens=len(chat_inputs)\n",
    "print(f\"Count Token:{num_tokens}\")\n",
    "print(f\"Token IDs:{chat_inputs}\")\n",
    "model_input_tokens=tokenizer.convert_ids_to_tokens(chat_inputs)\n",
    "print(model_input_tokens,end='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting eos_token is not supported, use the default one.\n",
      "Setting pad_token is not supported, use the default one.\n",
      "Setting unk_token is not supported, use the default one.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Human:你好啊\n",
      "==============\n",
      "Human message with history tokens:9\n",
      "AI:你好！有什么我可以帮助你的吗？\n",
      "===================\n",
      "AI response tokens:21\n",
      "*************\n",
      "Human:没有回复?\n",
      "==============\n",
      "Human message with history tokens:28\n",
      "AI:很抱歉，我之前的回复没有成功发送。如果你有任何问题或需要帮助，请随时告诉我，我会尽力帮助你。\n",
      "===================\n",
      "AI response tokens:39\n",
      "*************\n",
      "Human:你得数据到啥时候呢?\n",
      "==============\n",
      "Human message with history tokens:68\n",
      "AI:作为一个AI，我的数据更新是持续进行的。我旨在提供最新和最准确的信息，但请注意，我的知识库可能不包括最新的发展或事件。如果你有具体的问题或需要最新的信息，请告诉我，我会尽力提供帮助。\n",
      "===================\n",
      "AI response tokens:67\n",
      "*************\n",
      "Human:告诉我10个关于程序员的笑话?并且用日语输出\n",
      "==============\n",
      "Human message with history tokens:138\n",
      "AI:当然可以，以下是10个关于程序员的笑话，用日语表达：\n",
      "\n",
      "1. プログラマーのワンジョンは、コメントを書かないとバグが出ないと信じている。\n",
      "\n",
      "2. あるプログラマーは、自分のコードを修正するのが面倒で、新しいプログラムを書くことにした。\n",
      "\n",
      "3. プログラマーは、常に「これは最後のバグです」と言っているが、それは決して最後ではない。\n",
      "\n",
      "4. プログラマーは、午後に寝ているのを「デバッグ」している。\n",
      "\n",
      "5. あるプログラマーは、自分のPCに「Enter」キーを押すと、彼の上司が現れるように設定した。\n",
      "\n",
      "6. プログラマーの言葉には、彼らが自分のコードを読む能力を疑っていることが分かる。\n",
      "\n",
      "7. あるプログラマーは、自分のPCに「Esc」キーを押すと、彼の上司が消えるように設定した。\n",
      "\n",
      "8. プログラマーは、常に自分のコードを修正するのが面倒で、他のプログラマーのコードを書き換えることにした。\n",
      "\n",
      "9. あるプログラマーは、自分のPCに「Ctrl + Alt + Delete」を押すと、彼の上司が解雇されるように設定した。\n",
      "\n",
      "10. プログラマーは、自分のコードを修正するのが面倒で、自分のPCを壊してしまった。\n",
      "\n",
      "希望你喜欢这些笑话！\n",
      "===================\n",
      "AI response tokens:496\n",
      "*************\n",
      "Human:日语看不懂。转换成中文\n",
      "==============\n",
      "Human message with history tokens:625\n",
      "AI:当然可以，以下是10个关于程序员的笑话，用中文表达：\n",
      "\n",
      "1. 程序员相信，只要不写注释，代码就不会出错。\n",
      "\n",
      "2. 有个程序员觉得修改自己的代码太麻烦，于是决定写一个新程序。\n",
      "\n",
      "3. 程序员总是说“这是最后一个bug”，但那绝对不是最后一个。\n",
      "\n",
      "4. 程序员下午睡觉，美其名曰“调试”。\n",
      "\n",
      "5. 有个程序员把自己的电脑设置成按“Enter”键就会出现他的上司。\n",
      "\n",
      "6. 从程序员的语言中可以看出，他们对自己的代码阅读能力表示怀疑。\n",
      "\n",
      "7. 有个程序员把自己的电脑设置成按“Esc”键上司就会消失。\n",
      "\n",
      "8. 程序员觉得修改自己的代码太麻烦，于是决定改写其他程序员的代码。\n",
      "\n",
      "9. 有个程序员把自己的电脑设置成按“Ctrl + Alt + Delete”上司就会被解雇。\n",
      "\n",
      "10. 程序员觉得修改自己的代码太麻烦，结果把自己的电脑弄坏了。\n",
      "\n",
      "希望你喜欢这些笑话！\n",
      "===================\n",
      "AI response tokens:252\n",
      "*************\n",
      "Human:把上面输出的笑话转换成中文\n",
      "==============\n",
      "Human message with history tokens:875\n",
      "AI:当然可以，以下是上面用日语表达的笑话的中文版本：\n",
      "\n",
      "1. 程序员相信，只要不写注释，代码就不会出错。\n",
      "\n",
      "2. 有个程序员觉得修改自己的代码太麻烦，于是决定写一个新程序。\n",
      "\n",
      "3. 程序员总是说“这是最后一个bug”，但那绝对不是最后一个。\n",
      "\n",
      "4. 程序员下午睡觉，美其名曰“调试”。\n",
      "\n",
      "5. 有个程序员把自己的电脑设置成按“Enter”键就会出现他的上司。\n",
      "\n",
      "6. 从程序员的语言中可以看出，他们对自己的代码阅读能力表示怀疑。\n",
      "\n",
      "7. 有个程序员把自己的电脑设置成按“Esc”键上司就会消失。\n",
      "\n",
      "8. 程序员觉得修改自己的代码太麻烦，于是决定改写其他程序员的代码。\n",
      "\n",
      "9. 有个程序员把自己的电脑设置成按“Ctrl + Alt + Delete”上司就会被解雇。\n",
      "\n",
      "10. 程序员觉得修改自己的代码太麻烦，结果把自己的电脑弄坏了。\n",
      "\n",
      "希望你喜欢这些笑话！\n",
      "===================\n",
      "AI response tokens:251\n",
      "*************\n",
      "Human:我想给一个 使用 AIGC 作为创业内容的公司。帮我取10个跟AIGC相关的公司名称。并且输出 中文和 英文两种名称\n",
      "==============\n",
      "Human message with history tokens:1150\n",
      "AI:以下是10个与AIGC（人工智能生成内容）相关的公司名称，包括中文名称和英文名称：\n",
      "\n",
      "1. 智创未来科技有限公司 - Future AI Creation Technology Co., Ltd.\n",
      "2. 智能创意引擎有限公司 - Intelligent Creative Engine Co., Ltd.\n",
      "3. 智能内容创新科技有限公司 - Intelligent Content Innovation Technology Co., Ltd.\n",
      "4. 智慧创作实验室 - Wisdom Creation Lab\n",
      "5. 智能内容生成器科技有限公司 - Intelligent Content Generator Technology Co., Ltd.\n",
      "6. 创意智能引擎有限公司 - Creative AI Engine Co., Ltd.\n",
      "7. 智能创作网络技术有限公司 - Intelligent Creation Network Technology Co., Ltd.\n",
      "8. 智慧内容创新科技有限公司 - Wisdom Content Innovation Technology Co., Ltd.\n",
      "9. 智能创作工作室 - Intelligent Creation Studio\n",
      "10. 创意智能实验室 - Creative AI Lab\n",
      "\n",
      "希望这些建议能够帮助你找到一个合适的公司名称！\n",
      "===================\n",
      "AI response tokens:243\n",
      "*************\n"
     ]
    }
   ],
   "source": [
    "from transformers import  AutoTokenizer\n",
    "from zhipuai import  ZhipuAI\n",
    "class ChatSession:\n",
    "    def __init__(self,max_tokens=8192):\n",
    "        self.client=ZhipuAI()\n",
    "        self.history=[]\n",
    "        self.tokenizer=AutoTokenizer.from_pretrained(\n",
    "            \"THUDM/chatglm3-6b\",\n",
    "            trust_remote_code=True,\n",
    "            encode_sepcial_tokens=True\n",
    "        )\n",
    "        self.max_tokens=max_tokens\n",
    "    \n",
    "    def calculate_tokens(self,inputs):\n",
    "        return len(tokenizer.apply_chat_template(\n",
    "            inputs,\n",
    "            add_generation_prompt=True,\n",
    "            tokenize=True\n",
    "        ))\n",
    "    \n",
    "    def chat(self):\n",
    "        while True:\n",
    "            user_input=input(\"You:\")\n",
    "            if user_input.lower() in [\"exit\",\"quit\"]:\n",
    "                break\n",
    "            self.history.append({\n",
    "                \"role\":\"user\",\n",
    "                \"content\":user_input\n",
    "            })\n",
    "            print(f\"Human:{user_input}\")\n",
    "            user_tokens=self.calculate_tokens(self.history)\n",
    "            print(\"==============\")\n",
    "            print(f\"Human message with history tokens:{user_tokens}\")\n",
    "\n",
    "            response=self.client.chat.completions.create(\n",
    "                model=\"glm-4\",\n",
    "                messages=self.history,\n",
    "                top_p=0.7,\n",
    "                temperature=0.9,\n",
    "                stream=False,\n",
    "                max_tokens=self.max_tokens,\n",
    "            )\n",
    "            reply=response.choices[0].message.content\n",
    "            print(f\"AI:{reply}\")\n",
    "            self.history.append({\n",
    "                \"role\":\"assistant\",\n",
    "                \"content\": reply\n",
    "            })\n",
    "            reply_tokens=self.calculate_tokens(self.history[-2:])\n",
    "            print(\"===================\")\n",
    "            print(f\"AI response tokens:{reply_tokens}\",end=\"\\n*************\\n\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "chat_session=ChatSession()\n",
    "chat_session.chat()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "glm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
