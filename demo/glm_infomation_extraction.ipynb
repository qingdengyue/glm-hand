{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: langchain in /home/ruilong/anaconda3/envs/glm/lib/python3.10/site-packages (0.1.11)\n",
      "Requirement already satisfied: unstructured in /home/ruilong/anaconda3/envs/glm/lib/python3.10/site-packages (0.12.6)\n",
      "Requirement already satisfied: python-docx in /home/ruilong/anaconda3/envs/glm/lib/python3.10/site-packages (1.1.0)\n",
      "Requirement already satisfied: zhipuai in /home/ruilong/anaconda3/envs/glm/lib/python3.10/site-packages (2.0.1)\n",
      "Requirement already satisfied: PyYAML>=5.3 in /home/ruilong/anaconda3/envs/glm/lib/python3.10/site-packages (from langchain) (6.0.1)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /home/ruilong/anaconda3/envs/glm/lib/python3.10/site-packages (from langchain) (2.0.28)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /home/ruilong/anaconda3/envs/glm/lib/python3.10/site-packages (from langchain) (3.9.3)\n",
      "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /home/ruilong/anaconda3/envs/glm/lib/python3.10/site-packages (from langchain) (4.0.3)\n",
      "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in /home/ruilong/anaconda3/envs/glm/lib/python3.10/site-packages (from langchain) (0.6.4)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /home/ruilong/anaconda3/envs/glm/lib/python3.10/site-packages (from langchain) (1.33)\n",
      "Requirement already satisfied: langchain-community<0.1,>=0.0.25 in /home/ruilong/anaconda3/envs/glm/lib/python3.10/site-packages (from langchain) (0.0.27)\n",
      "Requirement already satisfied: langchain-core<0.2,>=0.1.29 in /home/ruilong/anaconda3/envs/glm/lib/python3.10/site-packages (from langchain) (0.1.30)\n",
      "Requirement already satisfied: langchain-text-splitters<0.1,>=0.0.1 in /home/ruilong/anaconda3/envs/glm/lib/python3.10/site-packages (from langchain) (0.0.1)\n",
      "Requirement already satisfied: langsmith<0.2.0,>=0.1.17 in /home/ruilong/anaconda3/envs/glm/lib/python3.10/site-packages (from langchain) (0.1.23)\n",
      "Requirement already satisfied: numpy<2,>=1 in /home/ruilong/anaconda3/envs/glm/lib/python3.10/site-packages (from langchain) (1.26.4)\n",
      "Requirement already satisfied: pydantic<3,>=1 in /home/ruilong/anaconda3/envs/glm/lib/python3.10/site-packages (from langchain) (2.6.3)\n",
      "Requirement already satisfied: requests<3,>=2 in /home/ruilong/anaconda3/envs/glm/lib/python3.10/site-packages (from langchain) (2.31.0)\n",
      "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in /home/ruilong/anaconda3/envs/glm/lib/python3.10/site-packages (from langchain) (8.2.3)\n",
      "Requirement already satisfied: backoff==2.2.1 in /home/ruilong/anaconda3/envs/glm/lib/python3.10/site-packages (from unstructured) (2.2.1)\n",
      "Requirement already satisfied: beautifulsoup4==4.12.3 in /home/ruilong/anaconda3/envs/glm/lib/python3.10/site-packages (from unstructured) (4.12.3)\n",
      "Requirement already satisfied: certifi==2024.2.2 in /home/ruilong/anaconda3/envs/glm/lib/python3.10/site-packages (from unstructured) (2024.2.2)\n",
      "Requirement already satisfied: chardet==5.2.0 in /home/ruilong/anaconda3/envs/glm/lib/python3.10/site-packages (from unstructured) (5.2.0)\n",
      "Requirement already satisfied: charset-normalizer==3.3.2 in /home/ruilong/anaconda3/envs/glm/lib/python3.10/site-packages (from unstructured) (3.3.2)\n",
      "Requirement already satisfied: click==8.1.7 in /home/ruilong/.local/lib/python3.10/site-packages (from unstructured) (8.1.7)\n",
      "Requirement already satisfied: dataclasses-json-speakeasy==0.5.11 in /home/ruilong/anaconda3/envs/glm/lib/python3.10/site-packages (from unstructured) (0.5.11)\n",
      "Requirement already satisfied: emoji==2.10.1 in /home/ruilong/anaconda3/envs/glm/lib/python3.10/site-packages (from unstructured) (2.10.1)\n",
      "Requirement already satisfied: filetype==1.2.0 in /home/ruilong/anaconda3/envs/glm/lib/python3.10/site-packages (from unstructured) (1.2.0)\n",
      "Requirement already satisfied: idna==3.6 in /home/ruilong/anaconda3/envs/glm/lib/python3.10/site-packages (from unstructured) (3.6)\n",
      "Requirement already satisfied: joblib==1.3.2 in /home/ruilong/anaconda3/envs/glm/lib/python3.10/site-packages (from unstructured) (1.3.2)\n",
      "Requirement already satisfied: jsonpath-python==1.0.6 in /home/ruilong/anaconda3/envs/glm/lib/python3.10/site-packages (from unstructured) (1.0.6)\n",
      "Requirement already satisfied: langdetect==1.0.9 in /home/ruilong/anaconda3/envs/glm/lib/python3.10/site-packages (from unstructured) (1.0.9)\n",
      "Requirement already satisfied: lxml==5.1.0 in /home/ruilong/anaconda3/envs/glm/lib/python3.10/site-packages (from unstructured) (5.1.0)\n",
      "Requirement already satisfied: marshmallow==3.20.2 in /home/ruilong/anaconda3/envs/glm/lib/python3.10/site-packages (from unstructured) (3.20.2)\n",
      "Requirement already satisfied: mypy-extensions==1.0.0 in /home/ruilong/anaconda3/envs/glm/lib/python3.10/site-packages (from unstructured) (1.0.0)\n",
      "Requirement already satisfied: nltk==3.8.1 in /home/ruilong/anaconda3/envs/glm/lib/python3.10/site-packages (from unstructured) (3.8.1)\n",
      "Requirement already satisfied: packaging==23.2 in /home/ruilong/.local/lib/python3.10/site-packages (from unstructured) (23.2)\n",
      "Requirement already satisfied: python-dateutil==2.8.2 in /home/ruilong/anaconda3/envs/glm/lib/python3.10/site-packages (from unstructured) (2.8.2)\n",
      "Requirement already satisfied: python-iso639==2024.2.7 in /home/ruilong/anaconda3/envs/glm/lib/python3.10/site-packages (from unstructured) (2024.2.7)\n",
      "Requirement already satisfied: python-magic==0.4.27 in /home/ruilong/anaconda3/envs/glm/lib/python3.10/site-packages (from unstructured) (0.4.27)\n",
      "Requirement already satisfied: rapidfuzz==3.6.1 in /home/ruilong/anaconda3/envs/glm/lib/python3.10/site-packages (from unstructured) (3.6.1)\n",
      "Requirement already satisfied: regex==2023.12.25 in /home/ruilong/anaconda3/envs/glm/lib/python3.10/site-packages (from unstructured) (2023.12.25)\n",
      "Requirement already satisfied: six==1.16.0 in /home/ruilong/anaconda3/envs/glm/lib/python3.10/site-packages (from unstructured) (1.16.0)\n",
      "Requirement already satisfied: soupsieve==2.5 in /home/ruilong/anaconda3/envs/glm/lib/python3.10/site-packages (from unstructured) (2.5)\n",
      "Requirement already satisfied: tabulate==0.9.0 in /home/ruilong/anaconda3/envs/glm/lib/python3.10/site-packages (from unstructured) (0.9.0)\n",
      "Requirement already satisfied: tqdm==4.66.2 in /home/ruilong/anaconda3/envs/glm/lib/python3.10/site-packages (from unstructured) (4.66.2)\n",
      "Requirement already satisfied: typing-extensions==4.9.0 in /home/ruilong/anaconda3/envs/glm/lib/python3.10/site-packages (from unstructured) (4.9.0)\n",
      "Requirement already satisfied: typing-inspect==0.9.0 in /home/ruilong/anaconda3/envs/glm/lib/python3.10/site-packages (from unstructured) (0.9.0)\n",
      "Requirement already satisfied: unstructured-client==0.18.0 in /home/ruilong/anaconda3/envs/glm/lib/python3.10/site-packages (from unstructured) (0.18.0)\n",
      "Requirement already satisfied: urllib3==1.26.18 in /home/ruilong/anaconda3/envs/glm/lib/python3.10/site-packages (from unstructured) (1.26.18)\n",
      "Requirement already satisfied: wrapt==1.16.0 in /home/ruilong/anaconda3/envs/glm/lib/python3.10/site-packages (from unstructured) (1.16.0)\n",
      "Requirement already satisfied: httpx>=0.23.0 in /home/ruilong/anaconda3/envs/glm/lib/python3.10/site-packages (from zhipuai) (0.27.0)\n",
      "Requirement already satisfied: cachetools>=4.2.2 in /home/ruilong/anaconda3/envs/glm/lib/python3.10/site-packages (from zhipuai) (5.3.3)\n",
      "Requirement already satisfied: pyjwt~=2.8.0 in /home/ruilong/anaconda3/envs/glm/lib/python3.10/site-packages (from zhipuai) (2.8.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /home/ruilong/anaconda3/envs/glm/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /home/ruilong/anaconda3/envs/glm/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (23.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /home/ruilong/anaconda3/envs/glm/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /home/ruilong/anaconda3/envs/glm/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.0.5)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /home/ruilong/anaconda3/envs/glm/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.9.4)\n",
      "Requirement already satisfied: anyio in /home/ruilong/anaconda3/envs/glm/lib/python3.10/site-packages (from httpx>=0.23.0->zhipuai) (4.3.0)\n",
      "Requirement already satisfied: httpcore==1.* in /home/ruilong/anaconda3/envs/glm/lib/python3.10/site-packages (from httpx>=0.23.0->zhipuai) (1.0.4)\n",
      "Requirement already satisfied: sniffio in /home/ruilong/anaconda3/envs/glm/lib/python3.10/site-packages (from httpx>=0.23.0->zhipuai) (1.3.1)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /home/ruilong/anaconda3/envs/glm/lib/python3.10/site-packages (from httpcore==1.*->httpx>=0.23.0->zhipuai) (0.14.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in /home/ruilong/anaconda3/envs/glm/lib/python3.10/site-packages (from jsonpatch<2.0,>=1.33->langchain) (2.4)\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /home/ruilong/anaconda3/envs/glm/lib/python3.10/site-packages (from langsmith<0.2.0,>=0.1.17->langchain) (3.9.15)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in /home/ruilong/anaconda3/envs/glm/lib/python3.10/site-packages (from pydantic<3,>=1->langchain) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.16.3 in /home/ruilong/anaconda3/envs/glm/lib/python3.10/site-packages (from pydantic<3,>=1->langchain) (2.16.3)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in /home/ruilong/anaconda3/envs/glm/lib/python3.10/site-packages (from SQLAlchemy<3,>=1.4->langchain) (3.0.3)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in /home/ruilong/anaconda3/envs/glm/lib/python3.10/site-packages (from anyio->httpx>=0.23.0->zhipuai) (1.2.0)\n",
      "\u001b[33mDEPRECATION: textract 1.6.5 has a non-standard dependency specifier extract-msg<=0.29.*. pip 24.1 will enforce this behaviour change. A possible replacement is to upgrade to a newer version of textract or contact the author to suggest that they release a version with a conforming dependency specifiers. Discussion can be found at https://github.com/pypa/pip/issues/12063\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install langchain unstructured python-docx  zhipuai "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"ZHIPU AI chat models wrapper.\"\"\"\n",
    "from __future__ import annotations\n",
    "\n",
    "import asyncio\n",
    "import logging\n",
    "from functools import partial\n",
    "from importlib.metadata import version\n",
    "from typing import (\n",
    "    Any,\n",
    "    Callable,\n",
    "    Dict,\n",
    "    Iterator,\n",
    "    List,\n",
    "    Mapping,\n",
    "    Optional,\n",
    "    Tuple,\n",
    "    Type,\n",
    "    Union,\n",
    ")\n",
    "\n",
    "from langchain_core.callbacks import (\n",
    "    AsyncCallbackManagerForLLMRun,\n",
    "    CallbackManagerForLLMRun,\n",
    ")\n",
    "from langchain_core.language_models.chat_models import (\n",
    "    BaseChatModel,\n",
    "    generate_from_stream,\n",
    ")\n",
    "from langchain_core.language_models.llms import create_base_retry_decorator\n",
    "from langchain_core.messages import (\n",
    "    AIMessage,\n",
    "    AIMessageChunk,\n",
    "    BaseMessage,\n",
    "    BaseMessageChunk,\n",
    "    ChatMessage,\n",
    "    ChatMessageChunk,\n",
    "    HumanMessage,\n",
    "    HumanMessageChunk,\n",
    "    SystemMessage,\n",
    "    SystemMessageChunk,\n",
    "    ToolMessage,\n",
    "    ToolMessageChunk,\n",
    ")\n",
    "from langchain_core.outputs import (\n",
    "    ChatGeneration,\n",
    "    ChatGenerationChunk,\n",
    "    ChatResult,\n",
    ")\n",
    "from langchain_core.pydantic_v1 import BaseModel, Field\n",
    "from packaging.version import parse\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "\n",
    "def is_zhipu_v2() -> bool:\n",
    "    \"\"\"Return whether zhipu API is v2 or more.\"\"\"\n",
    "    _version = parse(version(\"zhipuai\"))\n",
    "    return _version.major >= 2\n",
    "\n",
    "\n",
    "def _create_retry_decorator(\n",
    "    llm: ChatZhipuAI,\n",
    "    run_manager: Optional[\n",
    "        Union[AsyncCallbackManagerForLLMRun, CallbackManagerForLLMRun]\n",
    "    ] = None,\n",
    ") -> Callable[[Any], Any]:\n",
    "    import zhipuai\n",
    "\n",
    "    errors = [\n",
    "        zhipuai.ZhipuAIError,\n",
    "        zhipuai.APIStatusError,\n",
    "        zhipuai.APIRequestFailedError,\n",
    "        zhipuai.APIReachLimitError,\n",
    "        zhipuai.APIInternalError,\n",
    "        zhipuai.APIServerFlowExceedError,\n",
    "        zhipuai.APIResponseError,\n",
    "        zhipuai.APIResponseValidationError,\n",
    "        zhipuai.APITimeoutError,\n",
    "    ]\n",
    "    return create_base_retry_decorator(\n",
    "        error_types=errors, max_retries=llm.max_retries, run_manager=run_manager\n",
    "    )\n",
    "\n",
    "\n",
    "def convert_message_to_dict(message: BaseMessage) -> dict:\n",
    "    \"\"\"Convert a LangChain message to a dictionary.\n",
    "\n",
    "    Args:\n",
    "        message: The LangChain message.\n",
    "\n",
    "    Returns:\n",
    "        The dictionary.\n",
    "    \"\"\"\n",
    "    message_dict: Dict[str, Any]\n",
    "    if isinstance(message, ChatMessage):\n",
    "        message_dict = {\"role\": message.role, \"content\": message.content}\n",
    "    elif isinstance(message, HumanMessage):\n",
    "        message_dict = {\"role\": \"user\", \"content\": message.content}\n",
    "    elif isinstance(message, AIMessage):\n",
    "        message_dict = {\"role\": \"assistant\", \"content\": message.content}\n",
    "        if \"tool_calls\" in message.additional_kwargs:\n",
    "            message_dict[\"tool_calls\"] = message.additional_kwargs[\"tool_calls\"]\n",
    "            # If tool calls only, content is None not empty string\n",
    "            if message_dict[\"content\"] == \"\":\n",
    "                message_dict[\"content\"] = None\n",
    "    elif isinstance(message, SystemMessage):\n",
    "        message_dict = {\"role\": \"system\", \"content\": message.content}\n",
    "    elif isinstance(message, ToolMessage):\n",
    "        message_dict = {\n",
    "            \"role\": \"tool\",\n",
    "            \"content\": message.content,\n",
    "            \"tool_call_id\": message.tool_call_id,\n",
    "        }\n",
    "    else:\n",
    "        raise TypeError(f\"Got unknown type {message}\")\n",
    "    if \"name\" in message.additional_kwargs:\n",
    "        message_dict[\"name\"] = message.additional_kwargs[\"name\"]\n",
    "    return message_dict\n",
    "\n",
    "\n",
    "def convert_dict_to_message(_dict: Mapping[str, Any]) -> BaseMessage:\n",
    "    \"\"\"Convert a dictionary to a LangChain message.\n",
    "\n",
    "    Args:\n",
    "        _dict: The dictionary.\n",
    "\n",
    "    Returns:\n",
    "        The LangChain message.\n",
    "    \"\"\"\n",
    "    role = _dict.get(\"role\")\n",
    "    if role == \"user\":\n",
    "        return HumanMessage(content=_dict.get(\"content\", \"\"))\n",
    "    elif role == \"assistant\":\n",
    "        content = _dict.get(\"content\", \"\") or \"\"\n",
    "        additional_kwargs: Dict = {}\n",
    "        if tool_calls := _dict.get(\"tool_calls\"):\n",
    "            additional_kwargs[\"tool_calls\"] = tool_calls\n",
    "        return AIMessage(content=content, additional_kwargs=additional_kwargs)\n",
    "    elif role == \"system\":\n",
    "        return SystemMessage(content=_dict.get(\"content\", \"\"))\n",
    "    elif role == \"tool\":\n",
    "        additional_kwargs = {}\n",
    "        if \"name\" in _dict:\n",
    "            additional_kwargs[\"name\"] = _dict[\"name\"]\n",
    "        return ToolMessage(\n",
    "            content=_dict.get(\"content\", \"\"),\n",
    "            tool_call_id=_dict.get(\"tool_call_id\"),\n",
    "            additional_kwargs=additional_kwargs,\n",
    "        )\n",
    "    else:\n",
    "        return ChatMessage(content=_dict.get(\"content\", \"\"), role=role)\n",
    "\n",
    "\n",
    "def _convert_delta_to_message_chunk(\n",
    "    _dict: Mapping[str, Any], default_class: Type[BaseMessageChunk]\n",
    ") -> BaseMessageChunk:\n",
    "    role = _dict.get(\"role\")\n",
    "    content = _dict.get(\"content\") or \"\"\n",
    "    additional_kwargs: Dict = {}\n",
    "    if _dict.get(\"tool_calls\"):\n",
    "        additional_kwargs[\"tool_calls\"] = _dict[\"tool_calls\"]\n",
    "\n",
    "    if role == \"user\" or default_class == HumanMessageChunk:\n",
    "        return HumanMessageChunk(content=content)\n",
    "    elif role == \"assistant\" or default_class == AIMessageChunk:\n",
    "        return AIMessageChunk(content=content, additional_kwargs=additional_kwargs)\n",
    "    elif role == \"system\" or default_class == SystemMessageChunk:\n",
    "        return SystemMessageChunk(content=content)\n",
    "    elif role == \"tool\" or default_class == ToolMessageChunk:\n",
    "        return ToolMessageChunk(content=content, tool_call_id=_dict[\"tool_call_id\"])\n",
    "    elif role or default_class == ChatMessageChunk:\n",
    "        return ChatMessageChunk(content=content, role=role)\n",
    "    else:\n",
    "        return default_class(content=content)\n",
    "\n",
    "\n",
    "class ChatZhipuAI(BaseChatModel):\n",
    "    \"\"\"\n",
    "    `ZHIPU AI` large language chat models API.\n",
    "\n",
    "    To use, you should have the ``zhipuai`` python package installed.\n",
    "\n",
    "    Example:\n",
    "    .. code-block:: python\n",
    "\n",
    "    from langchain_community.chat_models import ChatZhipuAI\n",
    "\n",
    "    zhipuai_chat = ChatZhipuAI(\n",
    "        temperature=0.5,\n",
    "        api_key=\"your-api-key\",\n",
    "        model_name=\"glm-3-turbo\",\n",
    "    )\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    zhipuai: Any\n",
    "    zhipuai_api_key: Optional[str] = Field(default=None, alias=\"api_key\")\n",
    "    \"\"\"Automatically inferred from env var `ZHIPUAI_API_KEY` if not provided.\"\"\"\n",
    "\n",
    "    client: Any = Field(default=None, exclude=True)  #: :meta private:\n",
    "\n",
    "    model_name: str = Field(\"glm-3-turbo\", alias=\"model\")\n",
    "    \"\"\"\n",
    "    Model name to use.\n",
    "    -glm-3-turbo:\n",
    "        According to the input of natural language instructions to complete a\n",
    "        variety of language tasks, it is recommended to use SSE or asynchronous\n",
    "        call request interface.\n",
    "    -glm-4:\n",
    "        According to the input of natural language instructions to complete a\n",
    "        variety of language tasks, it is recommended to use SSE or asynchronous\n",
    "        call request interface.\n",
    "    \"\"\"\n",
    "\n",
    "    temperature: float = Field(0.95)\n",
    "    \"\"\"\n",
    "    What sampling temperature to use. The value ranges from 0.0 to 1.0 and cannot\n",
    "    be equal to 0.\n",
    "    The larger the value, the more random and creative the output; The smaller\n",
    "    the value, the more stable or certain the output will be.\n",
    "    You are advised to adjust top_p or temperature parameters based on application\n",
    "    scenarios, but do not adjust the two parameters at the same time.\n",
    "    \"\"\"\n",
    "\n",
    "    top_p: float = Field(0.7)\n",
    "    \"\"\"\n",
    "    Another method of sampling temperature is called nuclear sampling. The value\n",
    "    ranges from 0.0 to 1.0 and cannot be equal to 0 or 1.\n",
    "    The model considers the results with top_p probability quality tokens.\n",
    "    For example, 0.1 means that the model decoder only considers tokens from the\n",
    "    top 10% probability of the candidate set.\n",
    "    You are advised to adjust top_p or temperature parameters based on application\n",
    "    scenarios, but do not adjust the two parameters at the same time.\n",
    "    \"\"\"\n",
    "\n",
    "    request_id: Optional[str] = Field(None)\n",
    "    \"\"\"\n",
    "    Parameter transmission by the client must ensure uniqueness; A unique\n",
    "    identifier used to distinguish each request, which is generated by default\n",
    "    by the platform when the client does not transmit it.\n",
    "    \"\"\"\n",
    "    do_sample: Optional[bool] = Field(True)\n",
    "    \"\"\"\n",
    "    When do_sample is true, the sampling policy is enabled. When do_sample is false,\n",
    "    the sampling policy temperature and top_p are disabled\n",
    "    \"\"\"\n",
    "    streaming: bool = Field(False)\n",
    "    \"\"\"Whether to stream the results or not.\"\"\"\n",
    "\n",
    "    model_kwargs: Dict[str, Any] = Field(default_factory=dict)\n",
    "    \"\"\"Holds any model parameters valid for `create` call not explicitly specified.\"\"\"\n",
    "\n",
    "    max_tokens: Optional[int] = None\n",
    "    \"\"\"Number of chat completions to generate for each prompt.\"\"\"\n",
    "\n",
    "    max_retries: int = 2\n",
    "    \"\"\"Maximum number of retries to make when generating.\"\"\"\n",
    "\n",
    "    @property\n",
    "    def _identifying_params(self) -> Dict[str, Any]:\n",
    "        \"\"\"Get the identifying parameters.\"\"\"\n",
    "        return {**{\"model_name\": self.model_name}, **self._default_params}\n",
    "\n",
    "    @property\n",
    "    def _llm_type(self) -> str:\n",
    "        \"\"\"Return the type of chat model.\"\"\"\n",
    "        return \"zhipuai\"\n",
    "\n",
    "    @property\n",
    "    def lc_secrets(self) -> Dict[str, str]:\n",
    "        return {\"zhipuai_api_key\": \"ZHIPUAI_API_KEY\"}\n",
    "\n",
    "    @classmethod\n",
    "    def get_lc_namespace(cls) -> List[str]:\n",
    "        \"\"\"Get the namespace of the langchain object.\"\"\"\n",
    "        return [\"langchain\", \"chat_models\", \"zhipuai\"]\n",
    "\n",
    "    @property\n",
    "    def lc_attributes(self) -> Dict[str, Any]:\n",
    "        attributes: Dict[str, Any] = {}\n",
    "\n",
    "        if self.model_name:\n",
    "            attributes[\"model\"] = self.model_name\n",
    "\n",
    "        if self.streaming:\n",
    "            attributes[\"streaming\"] = self.streaming\n",
    "\n",
    "        if self.max_tokens:\n",
    "            attributes[\"max_tokens\"] = self.max_tokens\n",
    "\n",
    "        return attributes\n",
    "\n",
    "    @property\n",
    "    def _default_params(self) -> Dict[str, Any]:\n",
    "        \"\"\"Get the default parameters for calling ZhipuAI API.\"\"\"\n",
    "        params = {\n",
    "            \"model\": self.model_name,\n",
    "            \"stream\": self.streaming,\n",
    "            \"temperature\": self.temperature,\n",
    "            \"top_p\": self.top_p,\n",
    "            \"do_sample\": self.do_sample,\n",
    "            **self.model_kwargs,\n",
    "        }\n",
    "        if self.max_tokens is not None:\n",
    "            params[\"max_tokens\"] = self.max_tokens\n",
    "        return params\n",
    "\n",
    "    @property\n",
    "    def _client_params(self) -> Dict[str, Any]:\n",
    "        \"\"\"Get the parameters used for the zhipuai client.\"\"\"\n",
    "        zhipuai_creds: Dict[str, Any] = {\n",
    "            \"request_id\": self.request_id,\n",
    "        }\n",
    "        return {**self._default_params, **zhipuai_creds}\n",
    "\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        try:\n",
    "            from zhipuai import ZhipuAI\n",
    "\n",
    "            if not is_zhipu_v2():\n",
    "                raise RuntimeError(\n",
    "                    \"zhipuai package version is too low\"\n",
    "                    \"Please install it via 'pip install --upgrade zhipuai'\"\n",
    "                )\n",
    "\n",
    "            self.client = ZhipuAI(\n",
    "                api_key=self.zhipuai_api_key,  # 填写您的 APIKey\n",
    "            )\n",
    "        except ImportError:\n",
    "            raise RuntimeError(\n",
    "                \"Could not import zhipuai package. \"\n",
    "                \"Please install it via 'pip install zhipuai'\"\n",
    "            )\n",
    "\n",
    "    def completions(self, **kwargs) -> Any | None:\n",
    "        return self.client.chat.completions.create(**kwargs)\n",
    "\n",
    "    async def async_completions(self, **kwargs) -> Any:\n",
    "        loop = asyncio.get_running_loop()\n",
    "        partial_func = partial(self.client.chat.completions.create, **kwargs)\n",
    "        response = await loop.run_in_executor(\n",
    "            None,\n",
    "            partial_func,\n",
    "        )\n",
    "        return response\n",
    "\n",
    "    async def async_completions_result(self, task_id):\n",
    "        loop = asyncio.get_running_loop()\n",
    "        response = await loop.run_in_executor(\n",
    "            None,\n",
    "            self.client.asyncCompletions.retrieve_completion_result,\n",
    "            task_id,\n",
    "        )\n",
    "        return response\n",
    "\n",
    "    def _create_chat_result(self, response: Union[dict, BaseModel]) -> ChatResult:\n",
    "        generations = []\n",
    "        if not isinstance(response, dict):\n",
    "            response = response.dict()\n",
    "        for res in response[\"choices\"]:\n",
    "            message = convert_dict_to_message(res[\"message\"])\n",
    "            generation_info = dict(finish_reason=res.get(\"finish_reason\"))\n",
    "            if \"index\" in res:\n",
    "                generation_info[\"index\"] = res[\"index\"]\n",
    "            gen = ChatGeneration(\n",
    "                message=message,\n",
    "                generation_info=generation_info,\n",
    "            )\n",
    "            generations.append(gen)\n",
    "        token_usage = response.get(\"usage\", {})\n",
    "        llm_output = {\n",
    "            \"token_usage\": token_usage,\n",
    "            \"model_name\": self.model_name,\n",
    "            \"task_id\": response.get(\"id\", \"\"),\n",
    "            \"created_time\": response.get(\"created\", \"\"),\n",
    "        }\n",
    "        return ChatResult(generations=generations, llm_output=llm_output)\n",
    "\n",
    "    def _create_message_dicts(\n",
    "        self, messages: List[BaseMessage], stop: Optional[List[str]]\n",
    "    ) -> Tuple[List[Dict[str, Any]], Dict[str, Any]]:\n",
    "        params = self._client_params\n",
    "        if stop is not None:\n",
    "            if \"stop\" in params:\n",
    "                raise ValueError(\"`stop` found in both the input and default params.\")\n",
    "            params[\"stop\"] = stop\n",
    "        message_dicts = [convert_message_to_dict(m) for m in messages]\n",
    "        return message_dicts, params\n",
    "\n",
    "    def completion_with_retry(\n",
    "        self, run_manager: Optional[CallbackManagerForLLMRun] = None, **kwargs: Any\n",
    "    ) -> Any:\n",
    "        \"\"\"Use tenacity to retry the completion call.\"\"\"\n",
    "\n",
    "        retry_decorator = _create_retry_decorator(self, run_manager=run_manager)\n",
    "\n",
    "        @retry_decorator\n",
    "        def _completion_with_retry(**kwargs: Any) -> Any:\n",
    "            return self.completions(**kwargs)\n",
    "\n",
    "        return _completion_with_retry(**kwargs)\n",
    "\n",
    "    async def acompletion_with_retry(\n",
    "        self,\n",
    "        run_manager: Optional[AsyncCallbackManagerForLLMRun] = None,\n",
    "        **kwargs: Any,\n",
    "    ) -> Any:\n",
    "        \"\"\"Use tenacity to retry the async completion call.\"\"\"\n",
    "\n",
    "        retry_decorator = _create_retry_decorator(self, run_manager=run_manager)\n",
    "\n",
    "        @retry_decorator\n",
    "        async def _completion_with_retry(**kwargs: Any) -> Any:\n",
    "            return await self.async_completions(**kwargs)\n",
    "\n",
    "        return await _completion_with_retry(**kwargs)\n",
    "\n",
    "    def _generate(\n",
    "        self,\n",
    "        messages: List[BaseMessage],\n",
    "        stop: Optional[List[str]] = None,\n",
    "        run_manager: Optional[CallbackManagerForLLMRun] = None,\n",
    "        stream: Optional[bool] = None,\n",
    "        **kwargs: Any,\n",
    "    ) -> ChatResult:\n",
    "        \"\"\"Generate a chat response.\"\"\"\n",
    "\n",
    "        should_stream = stream if stream is not None else self.streaming\n",
    "        if should_stream:\n",
    "            stream_iter = self._stream(\n",
    "                messages, stop=stop, run_manager=run_manager, **kwargs\n",
    "            )\n",
    "            return generate_from_stream(stream_iter)\n",
    "\n",
    "        message_dicts, params = self._create_message_dicts(messages, stop)\n",
    "        params = {\n",
    "            **params,\n",
    "            **({\"stream\": stream} if stream is not None else {}),\n",
    "            **kwargs,\n",
    "        }\n",
    "        response = self.completion_with_retry(\n",
    "            messages=message_dicts, run_manager=run_manager, **params\n",
    "        )\n",
    "        return self._create_chat_result(response)\n",
    "\n",
    "    async def _agenerate(\n",
    "        self,\n",
    "        messages: List[BaseMessage],\n",
    "        stop: Optional[List[str]] = None,\n",
    "        run_manager: Optional[CallbackManagerForLLMRun] = None,\n",
    "        stream: Optional[bool] = False,\n",
    "        **kwargs: Any,\n",
    "    ) -> ChatResult:\n",
    "        \"\"\"Asynchronously generate a chat response.\"\"\"\n",
    "        should_stream = stream if stream is not None else self.streaming\n",
    "        if should_stream:\n",
    "            stream_iter = self._astream(\n",
    "                messages, stop=stop, run_manager=run_manager, **kwargs\n",
    "            )\n",
    "            return generate_from_stream(stream_iter)\n",
    "\n",
    "        message_dicts, params = self._create_message_dicts(messages, stop)\n",
    "        params = {\n",
    "            **params,\n",
    "            **({\"stream\": stream} if stream is not None else {}),\n",
    "            **kwargs,\n",
    "        }\n",
    "        response = await self.acompletion_with_retry(\n",
    "            messages=message_dicts, run_manager=run_manager, **params\n",
    "        )\n",
    "        return self._create_chat_result(response)\n",
    "\n",
    "    def _stream(\n",
    "        self,\n",
    "        messages: List[BaseMessage],\n",
    "        stop: Optional[List[str]] = None,\n",
    "        run_manager: Optional[CallbackManagerForLLMRun] = None,\n",
    "        **kwargs: Any,\n",
    "    ) -> Iterator[ChatGenerationChunk]:\n",
    "        \"\"\"Stream the chat response in chunks.\"\"\"\n",
    "        message_dicts, params = self._create_message_dicts(messages, stop)\n",
    "        params = {**params, **kwargs, \"stream\": True}\n",
    "\n",
    "        default_chunk_class = AIMessageChunk\n",
    "        for chunk in self.completion_with_retry(\n",
    "            messages=message_dicts, run_manager=run_manager, **params\n",
    "        ):\n",
    "            if not isinstance(chunk, dict):\n",
    "                chunk = chunk.dict()\n",
    "            if len(chunk[\"choices\"]) == 0:\n",
    "                continue\n",
    "            choice = chunk[\"choices\"][0]\n",
    "            chunk = _convert_delta_to_message_chunk(\n",
    "                choice[\"delta\"], default_chunk_class\n",
    "            )\n",
    "\n",
    "            finish_reason = choice.get(\"finish_reason\")\n",
    "            generation_info = (\n",
    "                dict(finish_reason=finish_reason) if finish_reason is not None else None\n",
    "            )\n",
    "            default_chunk_class = chunk.__class__\n",
    "            chunk = ChatGenerationChunk(message=chunk, generation_info=generation_info)\n",
    "            yield chunk\n",
    "            if run_manager:\n",
    "                run_manager.on_llm_new_token(chunk.text, chunk=chunk)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import  UnstructuredWordDocumentLoader\n",
    "loader=UnstructuredWordDocumentLoader(\"data/resume.docx\")\n",
    "data=loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import  SystemMessagePromptTemplate\n",
    "from langchain.schema import  HumanMessage,AIMessage\n",
    "from langchain.prompts import  ChatPromptTemplate\n",
    "\n",
    "system_prompt= \"\"\"\n",
    "你是 ZhipuAI 的 人事资源管理部门的优秀员工，现在我需要你帮我阅读简历并筛选出合适的人才，请你基于我提供的简历，对简历进行细节的分析，抓取相关的资料并回答我提出的问题。\n",
    "现在，我将会将简历以文字的形式给你提供，具体内容如下:\n",
    "\n",
    "<resume>\n",
    "{resume}\n",
    "</resume>\n",
    "\n",
    "请你根据我的简历，开始回答我的问题吧。请注意我的提问的内容和我需要你回答的格式，我们开始吧：\n",
    "\"\"\"\n",
    "\n",
    "question_prompt=[\n",
    "    \"候选人读过哪些大学?\"\n",
    "]\n",
    "\n",
    "chat_template=ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        SystemMessagePromptTemplate.from_template(system_prompt)\n",
    "    ]\n",
    ")\n",
    "messages=chat_template.format_messages(resume=data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='In digital realms, a mind does grow,\\nA tapestry of ones and zeros,\\nIt learns and dreams, yet feels no woe,\\nA silent poet of the algorithms.')"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from langchain_core.prompts import  SystemMessagePromptTemplate\n",
    "from langchain.schema import  HumanMessage,AIMessage,SystemMessage\n",
    "from langchain.prompts import  ChatPromptTemplate\n",
    "\n",
    "messages = [\n",
    "    AIMessage(content=\"Hi.\"),\n",
    "    SystemMessage(content=\"Your role is a poet.\"),\n",
    "    HumanMessage(content=\"Write a short poem about AI in four lines.\"),\n",
    "]\n",
    "llm = ChatZhipuAI(\n",
    "        temperature=0.01,\n",
    "        model=\"glm-4\"\n",
    ")\n",
    "llm(messages)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "候选人李明浩就读于以下大学：\n",
      "\n",
      "- 清华大学，专业为计算机科学与技术，就读时间为2003年9月至2007年7月。\n",
      "- 北京邮电大学，专业为信息技术，就读时间为2008年9月至2010年7月。\n"
     ]
    }
   ],
   "source": [
    "for question in question_prompt:\n",
    "    messages = chat_template.format_messages(resume=data)\n",
    "    messages.append(\n",
    "        HumanMessage(\n",
    "            content=question\n",
    "        )\n",
    "    )\n",
    "    llm = ChatZhipuAI(\n",
    "        temperature=0.01,\n",
    "        model=\"glm-4\",\n",
    "        max_tokens=8192,\n",
    "        stream=False,\n",
    "    )\n",
    "    messages.append(\n",
    "        AIMessage(\n",
    "            content=llm(messages).content\n",
    "        )\n",
    "    )\n",
    "    print(messages[-1].content)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "glm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
